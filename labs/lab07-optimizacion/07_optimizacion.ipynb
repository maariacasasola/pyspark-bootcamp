{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058355c2",
   "metadata": {},
   "source": [
    "# Optimización de Procesos en Spark\n",
    "\n",
    "Spark proporciona varias técnicas de optimización que permiten mejorar la eficiencia del procesamiento distribuido:\n",
    "\n",
    "- Catalyst Optimizer: motor de optimización para operaciones SQL y DataFrames.\n",
    "- Tungsten: optimización a nivel de memoria y CPU.\n",
    "- Particiones y persistencia: control de ejecución y uso eficiente de recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53273cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark07\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2889f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de ejemplo\n",
    "data = [\n",
    "    (\"Alice\", \"HR\", 3000),\n",
    "    (\"Bob\", \"IT\", 4500),\n",
    "    (\"Cathy\", \"HR\", 3200),\n",
    "    (\"David\", \"IT\", 5000),\n",
    "    (\"Eve\", \"Finance\", 4000),\n",
    "    (\"Frank\", \"Finance\", 4200)\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta optimizada automáticamente\n",
    "query = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) \n",
    "    FROM employees \n",
    "    WHERE salary > 3000 \n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439cb2a",
   "metadata": {},
   "source": [
    "## Cache y persistencia\n",
    "\n",
    "Spark permite mantener en memoria los resultados intermedios para evitar su recálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(df.salary > 3000)\n",
    "df_filtered.cache()\n",
    "df_filtered.count()  # Primera acción: carga en caché\n",
    "df_filtered.show()   # Segunda acción: uso desde caché"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee140474",
   "metadata": {},
   "source": [
    "## Particiones\n",
    "\n",
    "Usando Spark, podemos particionar los datos por una o más columnas. Cuando particionamos un conjunto de datos, lo estamos dividiendo en varios archivos. Por lo tanto, podemos leer solo una partición relevante cuando nos interese, y no todos los datos.\n",
    "Un ejemplo útil de dividir nuestros datos sería por la columna 'año', o las columnas 'año' y 'mes', si corresponde. De esta manera, podríamos acceder a un conjunto de datos correspondientes a una fecha específica.\n",
    "\n",
    "- `repartition(n)`: redistribuye los datos con `n` particiones (shuffle).\n",
    "- `coalesce(n)`: reduce el número de particiones sin shuffle (eficiente).\n",
    "- `partitionBy(column)`: redistribuye los datos sesgún los posibles valores de una columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb03baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el schema\n",
    "schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True),\n",
    "    StructField(\"views\", IntegerType(), True),\n",
    "    StructField(\"quality\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leer archivo CSV\n",
    "movies = spark.read.option(\"header\", True).schema(schema).csv(\"../../data/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados en particiones\n",
    "movies.write.partitionBy(\"quality\").parquet(\"data/partition_by_quality.parquet\")\n",
    "# Leer una de las particiones guardadas\n",
    "medium_quality_data = spark.read.parquet(\"data/partition_by_quality.parquet/quality=medium\")\n",
    "medium_quality_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d479e",
   "metadata": {},
   "source": [
    "### Formato Parquet\n",
    "Un archivo Parquet es un formato de archivo columnar utilizado para almacenar datos de una manera eficiente y optimizada. En lugar de guardar los datos en filas (como un archivo CSV o una tabla tradicional), Parquet los organiza por columnas, lo que permite consultas y análisis más rápidos, especialmente cuando se manejan grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420cf9e",
   "metadata": {},
   "source": [
    "## Buenas prácticas\n",
    "\n",
    "✅ Usar DataFrames en vez de RDDs para aprovechar Catalyst.  \n",
    "✅ Evitar `collect()` en grandes volúmenes.  \n",
    "✅ Revisar el plan de ejecución con `.explain()`.  \n",
    "✅ Usar `cache()` o `persist()` si reutilizas resultados.  \n",
    "✅ Ajustar particiones en base al volumen de datos y núcleos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be34ab7",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Leemos el dataset '../../data/movies.csv' y contamos el número de particiones.\n",
    "2. Reparticionar a 8 particiones, realizar una transformación y cachear el resultado.\n",
    "3. Usar `.explain()` para analizar el plan de ejecución antes y después del caché.\n",
    "4. Aplicar una función Window (lab04) para calcular el top 3 de películas según visualizaciones para cada tipo de calidad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f8c9b",
   "metadata": {},
   "source": [
    "#### Apartado 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d86763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = spark.read.option(\"header\", True).csv(\"../../data/movies.csv\")\n",
    "df.show(5)\n",
    "print('Número de particiones inciales:', df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3aa58",
   "metadata": {},
   "source": [
    "#### Apartado 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = df.repartition(8)\n",
    "filtered = partitions.filter('views > 10000')\n",
    "filtered.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0439c35",
   "metadata": {},
   "source": [
    "#### Apartado 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172dea0",
   "metadata": {},
   "source": [
    "Antes de filtrar y guardar en caché:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e730f",
   "metadata": {},
   "source": [
    "¿Qué está pasando?\n",
    "- Lectura directa del CSV: No hay filtros ni transformaciones. Spark está leyendo el archivo tal cual.\n",
    "- Reparticionado (`RoundRobin 8`): Los datos se redistribuyen equitativamente entre 8 particiones.\n",
    "- `AdaptiveSparkPlan isFinalPlan=false`: Spark ha planificado adaptativamente, pero aún no ha ejecutado la consulta, por lo que el plan podría cambiar en tiempo de ejecución (AQE en acción).\n",
    "- Sin caché: Cada vez que ejecutes una acción sobre este DataFrame, Spark leerá nuevamente desde el disco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e2695",
   "metadata": {},
   "source": [
    "Después de aplicar filtros y guardar en caché:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5216b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28531269",
   "metadata": {},
   "source": [
    "¿Qué significa?\n",
    "- Filtro aplicado:\n",
    "    - Spark filtra los registros `Filter (isnotnull(views#416) AND (cast(views#416 as int) > 10000))`.\n",
    "- Reparticionado después del filtro: Se realiza una redistribución de datos en 8 particiones después de aplicar el filtro.\n",
    "- `InMemoryRelation`:\n",
    "    - El DataFrame fue cacheado, así que ahora está en memoria (deserializado) y disponible con respaldo en disco si hace falta.\n",
    "- `InMemoryTableScan`:\n",
    "    - Las acciones siguientes a la caché ya no leerán desde el CSV, sino directamente desde memoria. Esto mejora significativamente el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f2928",
   "metadata": {},
   "source": [
    "#### Apartado 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378393ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar función de ventana: Top 3 películas por calidad según las visualizaciones\n",
    "window_spec = Window.partitionBy(\"quality\").orderBy(col(\"views\").desc())\n",
    "df_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "df_top3 = df_with_rank.filter(col(\"rank\") <= 3)\n",
    "df_top3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afbe8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
