{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1577070",
   "metadata": {},
   "source": [
    "## Soluciones lab05-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89ab23",
   "metadata": {},
   "source": [
    "Apartado 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"lab05\").getOrCreate()\n",
    "\n",
    "# Cargar datos\n",
    "taxi_zones = spark.read.option(\"header\", True).csv(\"../../data/taxi_zone_lookup.csv\")\n",
    "trips_df = spark.read.parquet(\"../../data/yellow_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5112a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones.createOrReplaceTempView(\"taxi_zones\")\n",
    "trips_df.createOrReplaceTempView(\"yellow_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones.show(5)\n",
    "trips_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d6e43",
   "metadata": {},
   "source": [
    "Apartado 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS pickup_zone, COUNT(*) AS num_trips\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY num_trips DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac1a19",
   "metadata": {},
   "source": [
    "Apartado 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS dropoff_zone, AVG(tip_amount) AS prom_tips\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.DOLocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY prom_tips DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b993",
   "metadata": {},
   "source": [
    "Apartado 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b34bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS pickup_zone, \n",
    "       AVG((unix_timestamp(t.tpep_dropoff_datetime) - unix_timestamp(t.tpep_pickup_datetime)) / 60.0) AS prom_trip_dur_min\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY prom_trip_dur_min DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680a539",
   "metadata": {},
   "source": [
    "¿Podríamos usar la siguiente consulta?\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS pickup_zone, \n",
    "    AVG(DATEDIFF(minute, tpep_pickup_datetime, tpep_dropoff_datetime)) AS prom_trip_dur_min\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY prom_trip_dur_min DESC\n",
    "\"\"\").show()\n",
    "```\n",
    "\n",
    "**En este entorno sí**, ya que probablemente esta consulta nos devuelva un resultado correcto y similar a la de arriba. Sin embargo, hay que tener en cuenta que este entorno Dockerizado está configurado a partir de una imagen predefinida (de Internet), por lo que seguramente cuente con alguna expensión adicional que añada un parsing flexible o compatibilidad extra.\n",
    "\n",
    "Comento esto porque es interesante saber que la forma \"canónica\" en Spark SQL es la primera, que usa el método explícito \"unix_timestamp()\". Ten en mente que si corres ese mismo código en Apache Spark sin extensiones (por ejemplo en un cluster o en PySpark CLI), puede fallar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba14ef",
   "metadata": {},
   "source": [
    "Apartado 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH viajes_franja AS (\n",
    "        SELECT z.Zone AS pickup_zone,\n",
    "        HOUR(t.tpep_pickup_datetime) AS hour, \n",
    "        CASE \n",
    "            WHEN hour BETWEEN 5 AND 11 THEN 'mañana'\n",
    "            WHEN hour BETWEEN 12 AND 17 THEN 'tarde'\n",
    "            WHEN hour BETWEEN 18 AND 23 THEN 'noche'\n",
    "            ELSE 'madrugada'\n",
    "            END \n",
    "        AS franja\n",
    "        FROM yellow_trips t\n",
    "        JOIN taxi_zones z ON t.PULocationID = z.LocationID\n",
    "    )\n",
    "    SELECT pickup_zone, \n",
    "    franja,\n",
    "    COUNT(*) AS num_trips\n",
    "    FROM viajes_franja\n",
    "    GROUP BY pickup_zone, franja\n",
    "    ORDER BY num_trips DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1db201",
   "metadata": {},
   "source": [
    "Apartado 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS pickup_zone, ROUND(SUM(total_amount), 2) AS total_income\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    HAVING count(*) > 10000\n",
    "    ORDER BY total_income DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2353e1",
   "metadata": {},
   "source": [
    "Apartado 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fef997",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS dropoff_zone, ROUND(STDDEV(t.tip_amount),2) as tip_var\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.DOLocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY tip_var DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
