{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c88e951",
   "metadata": {},
   "source": [
    "# Introducción a Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda41d99",
   "metadata": {},
   "source": [
    "\n",
    "Apache Spark es un motor de procesamiento de datos distribuido diseñado para ser rápido y general. Se utiliza principalmente para tareas de Big Data como análisis en batch, streaming, Machine Learning y procesamiento de gráficos.\n",
    "\n",
    "Spark permite dividir grandes volúmenes de datos en múltiples particiones y procesarlos en paralelo a través de un clúster.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859e837",
   "metadata": {},
   "source": [
    "## Arquitectura General de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21f738",
   "metadata": {},
   "source": [
    "\n",
    "La arquitectura de Spark se compone de los siguientes elementos:\n",
    "\n",
    "- **Driver Program**: el programa principal que ejecuta tu código Spark. Crea la `SparkSession`, genera el DAG, y coordina las tareas.\n",
    "- **Cluster Manager**: gestiona los recursos del clúster (YARN, Kubernetes, Mesos o Spark Standalone).\n",
    "- **Workers**: nodos que ejecutan las tareas en paralelo.\n",
    "- **Executors**: procesos que corren en los Workers y ejecutan el código.\n",
    "\n",
    "Spark divide un trabajo en múltiples **stages** que contienen múltiples **tasks**. Estos se planifican en un **DAG (Directed Acyclic Graph)** de operaciones.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1b338",
   "metadata": {},
   "source": [
    "## Iniciar una SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7308b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark01\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6c1b9",
   "metadata": {},
   "source": [
    "## Leer y mostrar un DataFrame simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2836af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f49a8c",
   "metadata": {},
   "source": [
    "## ¿Qué sucede en el fondo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104e5c8",
   "metadata": {},
   "source": [
    "\n",
    "Cada operación en Spark construye un DAG de transformaciones. Cuando se ejecuta una acción como `.show()` o `.collect()`, Spark planifica y ejecuta tareas distribuidas.\n",
    "\n",
    "Puedes visualizar el plan de ejecución con `.explain()`:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632e6bf",
   "metadata": {},
   "source": [
    "| Plan                       | ¿Qué muestra?                                                   |\n",
    "| -------------------------- | --------------------------------------------------------------- |\n",
    "| **Parsed Logical Plan**    | Tu código interpretado como operaciones lógicas                 |\n",
    "| **Analyzed Logical Plan**  | Añade tipos de datos, validación de columnas                    |\n",
    "| **Optimized Logical Plan** | Aplicación del Catalyst Optimizer (filtros, proyecciones, etc.) |\n",
    "| **Physical Plan**          | Lo que Spark realmente ejecuta (por etapas y workers)           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7977e70",
   "metadata": {},
   "source": [
    "_.explain(mode=\"...\")_ para más control\n",
    "\n",
    "Modos disponibles:\n",
    "- \"simple\": (por defecto) solo el plan físico\n",
    "- \"extended\": todos los niveles (parsed, analyzed, optimized, physical)\n",
    "- \"codegen\": código Java generado por Spark (útil para perf tuning avanzado)\n",
    "- \"cost\": incluye costos estimados de cada etapa (experimental)\n",
    "- \"formatted\": versión tabulada y más legible del extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e793c",
   "metadata": {},
   "source": [
    "La salidad de la función `explain()` se lee de abajo hacia arriba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c0f47",
   "metadata": {},
   "source": [
    "## Detener la SparkSession inciada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400b6c5",
   "metadata": {},
   "source": [
    "Finalmente debemos detener la sesión de Spark. Eso significa que liberamos todos los recursos asignados a nuestra sesión de Spark (memoria, conexiones, pojects, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572427",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
