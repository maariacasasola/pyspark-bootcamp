{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2838481f",
   "metadata": {},
   "source": [
    "# Spark SQL y vistas temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c833a",
   "metadata": {},
   "source": [
    "Spark SQL es el módulo de Apache Spark para trabajar con datos estructurados (esquema definido). Permite ejecutar consultas SQL o usar una API estilo SQL (como .select, .where, etc.) sobre DataFrames, tanto en lenguaje SQL como en APIs de alto nivel (PySpark, Scala, etc.). En este laboratorio, nos centraremos únicamente en consultas mediante lenguaje SQL y la creación de vistas temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb6afd",
   "metadata": {},
   "source": [
    "### Preparación de entorno y análisis de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"PySpark03\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = spark.read.option(\"header\", True).csv(\"../../data/taxi_zone_lookup.csv\")\n",
    "\n",
    "# Ver las primeras filas\n",
    "df.show(5)\n",
    "\n",
    "# Esquema del DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d07e4e",
   "metadata": {},
   "source": [
    "## Vistas temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5c361",
   "metadata": {},
   "source": [
    "Las vistas temporales (temp views) son una forma de registrar un DataFrame como una tabla SQL temporal en la sesión de Spark. Esto permite usar SQL sobre cualquier DataFrame que ya hayas cargado o transformado. Esto no guarda nada en disco, y solo vive mientras dure la sesión de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da639d",
   "metadata": {},
   "source": [
    "Existe una variante que no se borra al cerrar la sesión, sino que se comparte entre sesiones Spark:\n",
    "```python\n",
    "df.createGlobalTempView(\"nombre_global\")\n",
    "spark.sql(\"SELECT * FROM global_temp.nombre_global\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999702cd",
   "metadata": {},
   "source": [
    "##### ¿Cuándo usar vistas temporales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7ca40",
   "metadata": {},
   "source": [
    "- Cuando vienes del mundo SQL y te es más natural escribir consultas.\n",
    "- Cuando quieres dividir la lógica de procesamiento en etapas (puedes crear una vista para cada paso).\n",
    "- Cuando compartes lógica entre scripts o notebooks y necesitas una \"tabla virtual\" común.\n",
    "- Cuando haces prototipado rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vista temporal\n",
    "df.createOrReplaceTempView(\"taxi_zones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ded845",
   "metadata": {},
   "source": [
    "## Funciones integradas y expresiones SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24d9db",
   "metadata": {},
   "source": [
    "PySpark te permite realizar consultas SQL directamente sobre las tablas creadas. Por ejemplo, si tratáramos de responder a la pregunta \"¿Cuántas zonas hay por borough?\", podríamos formular la siguiente query:\n",
    "```sql SELECT Borough, COUNT(DISTINCT Zone) AS num_zones\n",
    "FROM taxi_zones\n",
    "GROUP BY Borough\n",
    "ORDER BY num_zones DESC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3267481",
   "metadata": {},
   "source": [
    "Con Spark, podemos lanzarla directamente como una query de  SQL sin tener que traducirla a PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT Borough, COUNT(DISTINCT Zone) AS num_zones\n",
    "    FROM taxi_zones\n",
    "    GROUP BY Borough\n",
    "    ORDER BY num_zones DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25616427",
   "metadata": {},
   "source": [
    "Ejemplo 2: \"¿Cuántos LocationID hay por zona de servicio?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a37b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT service_zone, COUNT(*) AS location_count\n",
    "    FROM taxi_zones\n",
    "    GROUP BY service_zone\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7835bb",
   "metadata": {},
   "source": [
    "Spark permite aplicar funciones como: AVG, COUNT, SUM, DATEDIFF, CASE WHEN, ROUND, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0a4b9",
   "metadata": {},
   "source": [
    "#### Función _.explain()_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601902b2",
   "metadata": {},
   "source": [
    "La función .explain() muestra el plan de ejecución de un DataFrame o consulta SQL.\n",
    "Este plan es como un \"manual de instrucciones\" que Spark sigue para ejecutar tu transformación o query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38b4a6",
   "metadata": {},
   "source": [
    "¿Para qué sirve?\n",
    "- Ver cómo Spark transforma tus operaciones lógicas en físicas.\n",
    "- Detectar si tus filtros y joins se están aplicando de forma eficiente.\n",
    "- Saber si Spark está usando el Catalyst Optimizer y el Tungsten engine correctamente.\n",
    "- Ayuda a detectar cuellos de botella o reprocesamientos innecesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5658d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT service_zone, COUNT(*) AS location_count\n",
    "    FROM taxi_zones\n",
    "    GROUP BY service_zone\n",
    "\"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203ca13",
   "metadata": {},
   "source": [
    "Por defecto _.explain()_ muestra el plan lógico y físico optimizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f150f2",
   "metadata": {},
   "source": [
    "| Plan                       | ¿Qué muestra?                                                   |\n",
    "| -------------------------- | --------------------------------------------------------------- |\n",
    "| **Parsed Logical Plan**    | Tu código interpretado como operaciones lógicas                 |\n",
    "| **Analyzed Logical Plan**  | Añade tipos de datos, validación de columnas                    |\n",
    "| **Optimized Logical Plan** | Aplicación del Catalyst Optimizer (filtros, proyecciones, etc.) |\n",
    "| **Physical Plan**          | Lo que Spark realmente ejecuta (por etapas y workers)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76696eb",
   "metadata": {},
   "source": [
    "##### _.explain(mode=\"...\")_ para más control\n",
    "```python\n",
    "df.explain(mode=\"formatted\")\n",
    "```\n",
    "Modos disponibles:\n",
    "- \"simple\": (por defecto) solo el plan físico\n",
    "- \"extended\": todos los niveles (parsed, analyzed, optimized, physical)\n",
    "- \"codegen\": código Java generado por Spark (útil para perf tuning avanzado)\n",
    "- \"cost\": incluye costos estimados de cada etapa (experimental)\n",
    "- \"formatted\": versión tabulada y más legible del extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT service_zone, COUNT(*) AS location_count\n",
    "    FROM taxi_zones\n",
    "    GROUP BY service_zone\n",
    "\"\"\").explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6597f7",
   "metadata": {},
   "source": [
    "### Join con Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e029b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de viajes\n",
    "trips_df = spark.read.parquet(\"../../data/yellow_tripdata_2023-01.parquet\")\n",
    "\n",
    "# Mostrar algunas filas\n",
    "trips_df.show(5)\n",
    "\n",
    "# Ver el esquema\n",
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262eea80",
   "metadata": {},
   "source": [
    "¿Cuáles son las zonas con más viajes iniciados? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"yellow_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS pickup_zone, COUNT(*) AS num_trips\n",
    "    FROM yellow_trips t\n",
    "    JOIN taxi_zones z\n",
    "    ON t.PULocationID = z.LocationID\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY num_trips DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ccf82",
   "metadata": {},
   "source": [
    "Detenemos la sesión de spark al finalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bed1128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
