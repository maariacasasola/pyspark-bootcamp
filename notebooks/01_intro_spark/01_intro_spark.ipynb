{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e7b2cd",
   "metadata": {},
   "source": [
    "# Get Started con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b9dc6",
   "metadata": {},
   "source": [
    "## Preparando el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e48cb",
   "metadata": {},
   "source": [
    "Vamos a importar los paquetes de python que son necesarios y abrir una sesión de Spark. Puedes nombrarla como quieras, elijo 'PySpark01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"PySpark01\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717ffd4",
   "metadata": {},
   "source": [
    "## Primeros pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76abab4",
   "metadata": {},
   "source": [
    "Estas son algunas transformaciones rápidas que PySpark te permite aplicar sobre tus datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d14622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el schema\n",
    "schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True),\n",
    "    StructField(\"views\", IntegerType(), True),\n",
    "    StructField(\"quality\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leer archivo CSV\n",
    "df = spark.read.option(\"header\", True).schema(schema).csv(\"../../data/movies.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Conteo total de registros\n",
    "print(\"Total de películas:\", df.count())\n",
    "\n",
    "# Filtrar y seleccionar columnas\n",
    "df_filtered = df.select(\"title\", \"genres\", \"views\", \"quality\").filter(col(\"views\") > 100000)\n",
    "\n",
    "# Agrupación por calidad\n",
    "df_grouped = df_filtered.groupBy(\"quality\").agg(avg(\"views\").alias(\"avg_views\"))\n",
    "df_grouped.orderBy(col(\"avg_views\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983be71",
   "metadata": {},
   "source": [
    "## Formato Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fe934",
   "metadata": {},
   "source": [
    "Un archivo Parquet es un formato de archivo columnar utilizado para almacenar datos de una manera eficiente y optimizada. En lugar de guardar los datos en filas (como un archivo CSV o una tabla tradicional), Parquet los organiza por columnas, lo que permite consultas y análisis más rápidos, especialmente cuando se manejan grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96295a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados en formato Parquet\n",
    "df_grouped.write.mode(\"overwrite\").parquet(\"data/popular_movies_by_quality.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145008b",
   "metadata": {},
   "source": [
    "## Particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57051d59",
   "metadata": {},
   "source": [
    "Usando Spark, podemos particionar los datos por una o más columnas. Cuando particionamos un conjunto de datos, lo estamos dividiendo en varios archivos. Por lo tanto, podemos leer solo una partición relevante cuando nos interese, y no todos los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed1f3f",
   "metadata": {},
   "source": [
    "Un buen ejemplo útil de dividir nuestros datos sería por la columna 'año', o las columnas 'año' y 'mes', si corresponde. De esta manera, podríamos acceder a un conjunto de datos correspondientes a una fecha específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados en particiones\n",
    "df.write.partitionBy(\"quality\").parquet(\"data/partition_by_quality.parquet\")\n",
    "# Leer una de las particiones guardadas\n",
    "medium_quality_data = spark.read.parquet(\"data/partition_by_quality.parquet/quality=medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicar función de ventana: Top 3 películas por calidad según las visualizaciones\n",
    "window_spec = Window.partitionBy(\"quality\").orderBy(col(\"views\").desc())\n",
    "df_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "df_top3 = df_with_rank.filter(col(\"rank\") <= 3)\n",
    "df_top3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092720c",
   "metadata": {},
   "source": [
    "## Detener la sesión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6a22c",
   "metadata": {},
   "source": [
    "Finalmente, como las mejores prácticas nos enseñan, debemos detener la sesión de Spark. Eso significa que liberamos todos los recursos asignados a nuestra sesión de Spark (memoria, conexiones, pojects, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detener la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
