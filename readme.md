# ğŸ§ª PySpark Bootcamp Lab

Este repositorio contiene laboratorios prÃ¡cticos diseÃ±ados para ayudarte a aprender a usar **Apache Spark con PySpark** en un entorno controlado y sin errores de configuraciÃ³n local. AquÃ­ encontrarÃ¡s ejercicios guiados, datasets, y un entorno Docker preconfigurado que evita los problemas tÃ­picos de instalaciÃ³n de Java, Spark o Hadoop en Windows.

---

## ğŸ“ InformaciÃ³n acerca del repositorio

Este proyecto contiene una serie de laboratorios educativos enfocados en la **ingenierÃ­a de datos**. El objetivo principal es facilitar el aprendizaje de tecnologÃ­as clave como:

- **Apache Spark / PySpark / Spark SQL**
- Formato de archivos **Parquet**
- ManipulaciÃ³n de datos con **DataFrames**
- EjecuciÃ³n en entornos **contenedorizados (Docker)**
- Buenas prÃ¡cticas en notebooks

El repositorio incluye notebooks, cÃ³digo de transformaciÃ³n, datos de ejemplo, y un entorno preconfigurado con Docker sobre Windows. La informaciÃ³n aportada en cada laboratorio se estructura de la siguiente manera:

| Laboratorio | TÃ­tulo | Detalle |
| ------------- | -------------------------- | ---------- |
| Lab 01 | **IntroducciÃ³n y arquitectura** | QuÃ© es Spark. Cluster vs. local. Driver, executor, worker, jobs, stages y tasks. ConfiguraciÃ³n de entorno: inicio y detenciÃ³n de sesiones |
| Lab 02 | **RDDs vs. DataFrames** | CÃ³mo se crean y manipulan. Ventajas y desventajas. Casos en los que aÃºn se usan RDDs |
| Lab 03 | **Transformaciones y acciones** | Lazy evaluation y su importancia. Transformaciones. Acciones. Aggregate. |
| Lab 04 | **Joins y ventanas** | Tipos de Joins. Funciones ventana. Uso en casos reales |
| Lab 05 | **Spark SQL** | Crear tablas temporales, consultas SQL. OptimizaciÃ³n con Catalyst |
| Lab 06 | **User Defined Functions (UDFs)** | Funciones definidas por el usuario (UDF) |
| Lab 07 | **OptimizaciÃ³n** | CachÃ© y persistencia. Reparticionamiento. Plan de ejecuciÃ³n. Escritura eficiente con Parquet y funciÃ³n explain() en profundidad |

## ğŸ§  Contenido actual
*Linaje de directiorios*
```
pyspark-bootcamp/
â”‚
â”œâ”€â”€ readme.mdâ”‚
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ [archivos CSV, Parquet de prÃ¡ctica]
â”‚
â””â”€â”€ labs/
    â”œâ”€â”€ 01_intro_spark/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â”œâ”€â”€ 02_rdds_vs_dataframes/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â”œâ”€â”€ 03_transformaciones_acciones/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â”œâ”€â”€ 04_ventanas_joins/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â”œâ”€â”€ 05_spark_sql_basico/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â”œâ”€â”€ 06_user_defined_functions/
    â”‚   â””â”€â”€ [contenido laboratorio]
    â””â”€â”€ 07_optimizacion/
        â””â”€â”€ [contenido laboratorio]
```

**data/**: contiene datasets con los datos de ejemplo que se usan en los distintos laboratorios.

**labs/**: contiene los directorios de los laboratorios que se usan para realizar este bootcamp.

Todos los laboratorios contienen un archivo *XX_lab_name.ipynb*, siendo XX el nÃºmero y lab_name el nombre especÃ­fico del laboratorio. AdemÃ¡s, la mayorÃ­a de ellos cuentan con un archivo *labXX-challenges.md* adicional con los enunciados de ejercicios propuestos para practicar lo aprendido en el laboratorio que corresponda. Finalmente, los archivos *labXX-solutions.ipynb* contienen las soluciones de dichos ejercicios propuestos.


### EjecuciÃ³n de Notebooks
Cada notebook estÃ¡ diseÃ±ado para ser ejecutado de forma independiente. Se recomienda seguir el orden numÃ©rico para una progresiÃ³n lÃ³gica del aprendizaje.

## âœ… Requisitos
Antes de comenzar, asegÃºrate de tener instalado lo siguiente:

- [Python 3.11](https://www.python.org/downloads/)
    - AÃ±adir a tus [variables de entorno](https://tecnoloco.istocks.club/como-agregar-python-a-la-variable-path-de-windows-wiki-util/2020-10-14/)
- [Visual Studio Code](https://code.visualstudio.com/download) con las extensiones:
    - ["Container Tools"](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-containers)
    - ["Dev Containers"](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker Desktop](https://www.docker.com/get-started/)
- [Git](https://git-scm.com/downloads) (para clonar el repositorio)

## ğŸš€ Pasos para ejecutar el entorno

1. **Ejecuta el siguiente comando para clonar este repositorio en git bash (o descarga el proyecto en .zip):**

   ```bash
   git clone https://github.com/tu-usuario/pyspark-bootcamp.git
   cd pyspark-bootcamp
   ```

2. **Ejecuta este comando para crear la imagen de Docker, si no estÃ¡ creada, y desplegar un contenedor a partir de ella:**

    ```bash
    docker compose up
    ```

3. **Navega a la pestaÃ±a 'Containers', previamente instalada en VSC**
4. **Haz clic derecho encima del contenedor desplegado y selecciona 'Attach Visual Studio Code' (esto abrirÃ¡ una nueva ventana de VSC en el entorno del contenedor)**
5. **Navega hasta /pyspark/labs/ y abre la carpeta del laboratorio que te interese para comenzar. Lo primero que debes hacer al abrir un notebook es seleccionar el kernel de Python (en mi caso Python 3.11)**

---

# ğŸ’¡ Conceptos teÃ³ricos

## Â¿QuÃ© es Apache Spark y cÃ³mo funciona internamente?
Apache Spark es un motor de procesamiento distribuido en memoria que permite ejecutar tareas de anÃ¡lisis de datos a gran escala. Su Ã©xito se debe a su velocidad, facilidad de uso y flexibilidad.

### Â¿CÃ³mo funciona Spark internamente?
RDDs (Resilient Distributed Datasets)
- Son la unidad de datos fundamental de Spark.
- Son inmutables, tolerantes a fallos, y se dividen en particiones que se distribuyen entre los nodos del clÃºster.
- Ya no se usan tanto directamente en PySpark, pero son la base sobre la que funcionan los DataFrames.

### DAG (Directed Acyclic Graph)
- Cada acciÃ³n en Spark (como collect(), write()) genera un plan de ejecuciÃ³n.
- Spark convierte tu cÃ³digo en una serie de transformaciones (como select, filter, groupBy), que forman un grafo dirigido acÃ­clico (DAG).
- El DAG Scheduler divide este grafo en etapas (stages) y tareas (tasks), que luego se ejecutan en paralelo.

### Catalyst Optimizer
- Es el optimizador de consultas de Spark SQL.
- Transforma el plan lÃ³gico en un "plan fÃ­sico" eficiente.
- Aplica reglas como predicado pushdown, reordenamiento de joins, etc.

### Tungsten Engine
- Motor de ejecuciÃ³n optimizado que usa:
 - GeneraciÃ³n de cÃ³digo en tiempo de ejecuciÃ³n (runtime code generation).
 - Manejo eficiente de memoria (evita objetos intermedios de JVM).
 - VectorizaciÃ³n de operaciones.
- Esto hace que Spark sea mucho mÃ¡s rÃ¡pido que Hadoop MapReduce.

## ğŸ§© PySpark: DataFrames, schema inference y transformaciones
### Â¿CÃ³mo funciona DataFrame en PySpark?
Es una colecciÃ³n distribuida de datos organizados en columnas (como una tabla de SQL o un DataFrame de pandas) construido sobre RDDs, pero con optimizaciÃ³n automÃ¡tica gracias a Catalyst.
Spark puede inferir automÃ¡ticamente los tipos de datos (inferSchema=True) o puedes especificarlos manualmente con StructType.


| CaracterÃ­stica                | pandas                 | PySpark                            |
| ----------------------------- | ---------------------- | ---------------------------------- |
| Escalabilidad                 | Memoria local          | Procesamiento distribuido          |
| Velocidad en grandes datasets | Lenta                  | Muy rÃ¡pida (paralelismo + memoria) |
| Lazy evaluation               | âŒ No                   | âœ… SÃ­                               |
| Tipado                        | DinÃ¡mico               | Inferido o explÃ­cito               |
| Sintaxis                      | Muy expresiva/flexible | MÃ¡s declarativa y estricta         |
| Uso tÃ­pico                    | ExploraciÃ³n local      | Procesamiento a gran escala        |

### Â¿CuÃ¡ndo usar cada uno?
- Pandas: Ideal para datasets < 1 GB, exploraciÃ³n rÃ¡pida.
- PySpark: Fundamental para Big Data (decenas o cientos de GB), procesamiento distribuido, producciÃ³n.

---

# ğŸ§‘â€ğŸ’» Autor
Creado y documentado por MarÃ­a Casasola como parte de un proceso de aprendizaje profesional en ingenierÃ­a de datos.
Para dudas, puedes contactarme vÃ­a [LinkedIn](https://www.linkedin.com/in/mar%C3%ADa-casasola-calzadilla-303970184/).

## ğŸ¤ Contribuciones
Este repositorio estÃ¡ en constante mejora. Si tienes ideas, correcciones o quieres colaborar, Â¡haz un fork y crea un pull request!
