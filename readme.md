# üß™ PySpark Bootcamp Lab

Este repositorio contiene laboratorios pr√°cticos dise√±ados para ayudarte a aprender a usar **Apache Spark con PySpark** en un entorno controlado y sin errores de configuraci√≥n local. Aqu√≠ encontrar√°s ejercicios guiados, datasets, y un entorno Docker preconfigurado que evita los problemas t√≠picos de instalaci√≥n de Java, Spark o Hadoop en Windows.

---

## üìÅ Informaci√≥n acerca del repositorio

Este proyecto contiene una serie de laboratorios educativos enfocados en la **ingenier√≠a de datos**. El objetivo principal es facilitar el aprendizaje de tecnolog√≠as clave como:

- **Apache Spark / PySpark**
- Formato de archivos **Parquet**
- Manipulaci√≥n de datos con **DataFrames**
- Ejecuci√≥n en entornos **contenedorizados (Docker)**
- Buenas pr√°cticas en notebooks

El repositorio incluye notebooks, c√≥digo de transformaci√≥n, datos de ejemplo, y un entorno preconfigurado con Docker que funciona perfectamente en Windows.

---

## ‚úÖ Requisitos

Antes de comenzar, aseg√∫rate de tener instalado lo siguiente:

- [Visual Studio Code](https://code.visualstudio.com/download) con la extensiones:
    - "Container Tools" (https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-containers)
    - "Dev Containers" (https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker Desktop](https://www.docker.com/get-started/)
- Git (para clonar el repositorio)

---


## üöÄ Pasos para ejecutar el entorno

1. **Clona este repositorio:**

   ```bash
   git clone https://github.com/tu-usuario/pyspark-bootcamp.git
   cd pyspark-bootcamp
   ```

2. **Crea la imagen de Docker si no est√° creada y despliega un contenedor a partir de ella:**

    ```bash
    docker compose up
    ```

3. **Navega a la pesta√±a de la extensi√≥n 'Dev Containers', previamente instalada en VSC**
4. **Haz clic derecho encima del contenedor desplegado y selecciona 'Attach in Visual Studio Code' (esto abrir√° una nueva ventana de VSC con el entorno del contenedor)**
5. **Navegar hasta /pyspark/notebooks/ y abrir el primer archivo de laboratorio "01_intro_spark.ipynb" para comenzar**

# Conceptos te√≥ricos
## ¬øQu√© es Apache Spark y c√≥mo funciona internamente?
Apache Spark es un motor de procesamiento distribuido en memoria que permite ejecutar tareas de an√°lisis de datos a gran escala. Su √©xito se debe a su velocidad, facilidad de uso y flexibilidad.

### ¬øC√≥mo funciona Spark internamente?
RDDs (Resilient Distributed Datasets)
- Son la unidad de datos fundamental de Spark.
- Son inmutables, tolerantes a fallos, y se dividen en particiones que se distribuyen entre los nodos del cl√∫ster.
- Ya no se usan tanto directamente en PySpark, pero son la base sobre la que funcionan los DataFrames.

### DAG (Directed Acyclic Graph)
- Cada acci√≥n en Spark (como collect(), write()) genera un plan de ejecuci√≥n.
- Spark convierte tu c√≥digo en una serie de transformaciones (como select, filter, groupBy), que forman un grafo dirigido ac√≠clico (DAG).
- El DAG Scheduler divide este grafo en etapas (stages) y tareas (tasks), que luego se ejecutan en paralelo.

### Catalyst Optimizer
- Es el optimizador de consultas de Spark SQL.
- Transforma el plan l√≥gico en plan f√≠sico eficiente.
- Aplica reglas como predicado pushdown, reordenamiento de joins, etc.

### Tungsten Engine
- Motor de ejecuci√≥n optimizado que usa:
 - Generaci√≥n de c√≥digo en tiempo de ejecuci√≥n (runtime code generation).
 - Manejo eficiente de memoria (evita objetos intermedios de JVM).
 - Vectorizaci√≥n de operaciones.
- Esto hace que Spark sea mucho m√°s r√°pido que Hadoop MapReduce.

## PySpark: DataFrames, schema inference y transformaciones
### ¬øC√≥mo funciona DataFrame en PySpark?
Es una colecci√≥n distribuida de datos organizados en columnas (como una tabla de SQL o un DataFrame de pandas) construido sobre RDDs, pero con optimizaci√≥n autom√°tica gracias a Catalyst.
Spark puede inferir autom√°ticamente los tipos de datos (inferSchema=True) o puedes especificarlos manualmente con StructType.


| Caracter√≠stica                | pandas                 | PySpark                            |
| ----------------------------- | ---------------------- | ---------------------------------- |
| Escalabilidad                 | Memoria local          | Procesamiento distribuido          |
| Velocidad en grandes datasets | Lenta                  | Muy r√°pida (paralelismo + memoria) |
| Lazy evaluation               | ‚ùå No                   | ‚úÖ S√≠                               |
| Tipado                        | Din√°mico               | Inferido o expl√≠cito               |
| Sintaxis                      | Muy expresiva/flexible | M√°s declarativa y estricta         |
| Uso t√≠pico                    | Exploraci√≥n local      | Procesamiento a gran escala        |

### ¬øCu√°ndo usar cada uno?
pandas: Ideal para datasets < 1 GB, exploraci√≥n r√°pida.
PySpark: Fundamental para Big Data (decenas o cientos de GB), procesamiento distribuido, producci√≥n