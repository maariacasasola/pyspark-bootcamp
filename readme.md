# üß™ PySpark Bootcamp Lab

Este repositorio contiene laboratorios pr√°cticos dise√±ados para ayudarte a aprender a usar **Apache Spark con PySpark** en un entorno controlado y sin errores de configuraci√≥n local. Aqu√≠ encontrar√°s ejercicios guiados, datasets, y un entorno Docker preconfigurado que evita los problemas t√≠picos de instalaci√≥n de Java, Spark o Hadoop en Windows.

---

## üìÅ Informaci√≥n acerca del repositorio

Este proyecto contiene una serie de laboratorios educativos enfocados en la **ingenier√≠a de datos**. El objetivo principal es facilitar el aprendizaje de tecnolog√≠as clave como:

- **Apache Spark / PySpark**
- Formato de archivos **Parquet**
- Manipulaci√≥n de datos con **DataFrames**
- Ejecuci√≥n en entornos **contenedorizados (Docker)**
- Buenas pr√°cticas en notebooks

El repositorio incluye notebooks, c√≥digo de transformaci√≥n, datos de ejemplo, y un entorno preconfigurado con Docker sobre Windows.

## üß† Contenido actual
**notebooks/**: este directorio contiene diferentes carpetas que dividen el aprendizaje por niveles o *laboratorios*, empezando por el 01. Dentro de cada *carpeta de laboratorio* encontrar√°s un archivo .ipynb que comienza por el n√∫mero de laboratorio al que pertenezca (coincide con el n√∫mero que aparece en el nombre de la carpeta), este *archivo explicativo* incluye explicaciones de los distintos m√©todos y funciones que se usen de PySpark. Adem√°s, tambi√©n se a√±ade el mismo c√≥digo en un archivo .py, para quien se sienta m√°s c√≥modo trabajando directamente en scripts de Python. Finalmente, en cada *carpeta de laboratorio* hay un archivo adicional .ipynb que comienza por "challenge-" donde se proponen una serie de ejercicios con sus resultados. Estos *retos* est√°n pensados tanto para poner en pr√°ctica la informaci√≥n observada en los *archivos explicativos* como para realizar un aprendizaje autodidacta buscando en Internet.  

**data/**: contiene datasets con los datos de ejemplo que se usan en los distintos laboratorios.

## ‚úÖ Requisitos

Antes de comenzar, aseg√∫rate de tener instalado lo siguiente:

- [Visual Studio Code](https://code.visualstudio.com/download) con las extensiones:
    - ["Container Tools"](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-containers)
    - ["Dev Containers"](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker Desktop](https://www.docker.com/get-started/)
- [Git](https://git-scm.com/downloads) (para clonar el repositorio)

## üöÄ Pasos para ejecutar el entorno

1. **Ejecuta el siguiente comando para clonar este repositorio en git bash (o descarga el proyecto en .zip):**

   ```bash
   git clone https://github.com/tu-usuario/pyspark-bootcamp.git
   cd pyspark-bootcamp
   ```

2. **Ejecuta este comando para crear la imagen de Docker, si no est√° creada, y desplegar un contenedor a partir de ella:**

    ```bash
    docker compose up
    ```

3. **Navega a la pesta√±a de la extensi√≥n 'Dev Containers', previamente instalada en VSC**
4. **Haz clic derecho encima del contenedor desplegado y selecciona 'Attach in Visual Studio Code' (esto abrir√° una nueva ventana de VSC en el entorno del contenedor)**
5. **Navega hasta /pyspark/notebooks/ y abre el primer archivo de laboratorio "01_intro_spark.ipynb" para comenzar.**

---

# üí° Conceptos te√≥ricos

## ¬øQu√© es Apache Spark y c√≥mo funciona internamente?
Apache Spark es un motor de procesamiento distribuido en memoria que permite ejecutar tareas de an√°lisis de datos a gran escala. Su √©xito se debe a su velocidad, facilidad de uso y flexibilidad.

### ¬øC√≥mo funciona Spark internamente?
RDDs (Resilient Distributed Datasets)
- Son la unidad de datos fundamental de Spark.
- Son inmutables, tolerantes a fallos, y se dividen en particiones que se distribuyen entre los nodos del cl√∫ster.
- Ya no se usan tanto directamente en PySpark, pero son la base sobre la que funcionan los DataFrames.

### DAG (Directed Acyclic Graph)
- Cada acci√≥n en Spark (como collect(), write()) genera un plan de ejecuci√≥n.
- Spark convierte tu c√≥digo en una serie de transformaciones (como select, filter, groupBy), que forman un grafo dirigido ac√≠clico (DAG).
- El DAG Scheduler divide este grafo en etapas (stages) y tareas (tasks), que luego se ejecutan en paralelo.

### Catalyst Optimizer
- Es el optimizador de consultas de Spark SQL.
- Transforma el plan l√≥gico en un "plan f√≠sico" eficiente.
- Aplica reglas como predicado pushdown, reordenamiento de joins, etc.

### Tungsten Engine
- Motor de ejecuci√≥n optimizado que usa:
 - Generaci√≥n de c√≥digo en tiempo de ejecuci√≥n (runtime code generation).
 - Manejo eficiente de memoria (evita objetos intermedios de JVM).
 - Vectorizaci√≥n de operaciones.
- Esto hace que Spark sea mucho m√°s r√°pido que Hadoop MapReduce.

## üß© PySpark: DataFrames, schema inference y transformaciones
### ¬øC√≥mo funciona DataFrame en PySpark?
Es una colecci√≥n distribuida de datos organizados en columnas (como una tabla de SQL o un DataFrame de pandas) construido sobre RDDs, pero con optimizaci√≥n autom√°tica gracias a Catalyst.
Spark puede inferir autom√°ticamente los tipos de datos (inferSchema=True) o puedes especificarlos manualmente con StructType.


| Caracter√≠stica                | pandas                 | PySpark                            |
| ----------------------------- | ---------------------- | ---------------------------------- |
| Escalabilidad                 | Memoria local          | Procesamiento distribuido          |
| Velocidad en grandes datasets | Lenta                  | Muy r√°pida (paralelismo + memoria) |
| Lazy evaluation               | ‚ùå No                   | ‚úÖ S√≠                               |
| Tipado                        | Din√°mico               | Inferido o expl√≠cito               |
| Sintaxis                      | Muy expresiva/flexible | M√°s declarativa y estricta         |
| Uso t√≠pico                    | Exploraci√≥n local      | Procesamiento a gran escala        |

### ¬øCu√°ndo usar cada uno?
- Pandas: Ideal para datasets < 1 GB, exploraci√≥n r√°pida.
- PySpark: Fundamental para Big Data (decenas o cientos de GB), procesamiento distribuido, producci√≥n.

---

# üßë‚Äçüíª Autor
Creado y documentado por Mar√≠a Casasola como parte de un proceso de aprendizaje profesional en ingenier√≠a de datos.
Para dudas, puedes contactarme v√≠a [LinkedIn](https://www.linkedin.com/in/mar%C3%ADa-casasola-calzadilla-303970184/).

## ü§ù Contribuciones
Este repositorio est√° en constante mejora. Si tienes ideas, correcciones o quieres colaborar, ¬°haz un fork y crea un pull request!