# ğŸ§ª PySpark Bootcamp Lab

Este repositorio contiene laboratorios prÃ¡cticos diseÃ±ados para ayudarte a aprender a usar **Apache Spark con PySpark** en un entorno controlado y sin errores de configuraciÃ³n local. AquÃ­ encontrarÃ¡s ejercicios guiados, datasets, y un entorno Docker preconfigurado que evita los problemas tÃ­picos de instalaciÃ³n de Java, Spark o Hadoop en Windows.

---

## ğŸ“ InformaciÃ³n acerca del repositorio

Este proyecto contiene una serie de laboratorios educativos enfocados en la **ingenierÃ­a de datos**. El objetivo principal es facilitar el aprendizaje de tecnologÃ­as clave como:

- **Apache Spark / PySpark / Spark SQL**
- Formato de archivos **Parquet**
- ManipulaciÃ³n de datos con **DataFrames**
- EjecuciÃ³n en entornos **contenedorizados (Docker)**
- Buenas prÃ¡cticas en notebooks

El repositorio incluye notebooks, cÃ³digo de transformaciÃ³n, datos de ejemplo, y un entorno preconfigurado con Docker sobre Windows.

## ğŸ§  Contenido actual
*Linaje de directiorios*
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ 01_lab_name
â”‚   â”‚   â”œâ”€â”€ 01_lab_name.ipynb
â”‚   â”‚   â”œâ”€â”€ 01_lab_name.py
â”‚   â”‚   â”œâ”€â”€ challenge-01_lab_name.ipynb
â”‚   â”œâ”€â”€ 02_lab_name
â”‚   â”‚   â”œâ”€â”€ 02_lab_name.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_lab_name.py
â”‚   â”‚   â”œâ”€â”€ challenge-02_lab_name.ipynb
â”‚   â”œâ”€â”€ 03_lab_name
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   ...

**data/**: contiene datasets con los datos de ejemplo que se usan en los distintos laboratorios.

**notebooks/**: este directorio contiene diferentes carpetas que dividen el aprendizaje por niveles o *laboratorios*, empezando por el 01. Dentro de cada *carpeta de laboratorio* encontrarÃ¡s un archivo .ipynb que comienza por el nÃºmero de laboratorio al que pertenezca (coincide con el nÃºmero que aparece en el nombre de la carpeta), este *archivo explicativo* incluye explicaciones de los distintos mÃ©todos y funciones que se usan. AdemÃ¡s, tambiÃ©n se aÃ±ade el mismo cÃ³digo en un archivo .py, para quien se sienta mÃ¡s cÃ³modo trabajando directamente en scripts de Python. Finalmente, en cada *carpeta de laboratorio* hay tambiÃ©n un archivo adicional .ipynb que comienza por "challenge-" donde se proponen una serie de ejercicios con sus resultados. Estos *retos* estÃ¡n pensados tanto para poner en prÃ¡ctica la informaciÃ³n observada en los *archivos explicativos*, para poder realizar un aprendizaje autodidacta buscando en Internet.  

## âœ… Requisitos

Antes de comenzar, asegÃºrate de tener instalado lo siguiente:

- [Visual Studio Code](https://code.visualstudio.com/download) con las extensiones:
    - ["Container Tools"](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-containers)
    - ["Dev Containers"](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
- [Docker Desktop](https://www.docker.com/get-started/)
- [Git](https://git-scm.com/downloads) (para clonar el repositorio)

## ğŸš€ Pasos para ejecutar el entorno

1. **Ejecuta el siguiente comando para clonar este repositorio en git bash (o descarga el proyecto en .zip):**

   ```bash
   git clone https://github.com/tu-usuario/pyspark-bootcamp.git
   cd pyspark-bootcamp
   ```

2. **Ejecuta este comando para crear la imagen de Docker, si no estÃ¡ creada, y desplegar un contenedor a partir de ella:**

    ```bash
    docker compose up
    ```

3. **Navega a la pestaÃ±a 'Containers', previamente instalada en VSC**
4. **Haz clic derecho encima del contenedor desplegado y selecciona 'Attach Visual Studio Code' (esto abrirÃ¡ una nueva ventana de VSC en el entorno del contenedor)**
5. **Navega hasta /pyspark/notebooks/ y abre la carpeta del laboratorio que te interese para comenzar. Lo primero que debes hacer al abrir un notebook es seleccionar el kernel de Python (en mi caso Python 3.11)**

---

# ğŸ’¡ Conceptos teÃ³ricos

## Â¿QuÃ© es Apache Spark y cÃ³mo funciona internamente?
Apache Spark es un motor de procesamiento distribuido en memoria que permite ejecutar tareas de anÃ¡lisis de datos a gran escala. Su Ã©xito se debe a su velocidad, facilidad de uso y flexibilidad.

### Â¿CÃ³mo funciona Spark internamente?
RDDs (Resilient Distributed Datasets)
- Son la unidad de datos fundamental de Spark.
- Son inmutables, tolerantes a fallos, y se dividen en particiones que se distribuyen entre los nodos del clÃºster.
- Ya no se usan tanto directamente en PySpark, pero son la base sobre la que funcionan los DataFrames.

### DAG (Directed Acyclic Graph)
- Cada acciÃ³n en Spark (como collect(), write()) genera un plan de ejecuciÃ³n.
- Spark convierte tu cÃ³digo en una serie de transformaciones (como select, filter, groupBy), que forman un grafo dirigido acÃ­clico (DAG).
- El DAG Scheduler divide este grafo en etapas (stages) y tareas (tasks), que luego se ejecutan en paralelo.

### Catalyst Optimizer
- Es el optimizador de consultas de Spark SQL.
- Transforma el plan lÃ³gico en un "plan fÃ­sico" eficiente.
- Aplica reglas como predicado pushdown, reordenamiento de joins, etc.

### Tungsten Engine
- Motor de ejecuciÃ³n optimizado que usa:
 - GeneraciÃ³n de cÃ³digo en tiempo de ejecuciÃ³n (runtime code generation).
 - Manejo eficiente de memoria (evita objetos intermedios de JVM).
 - VectorizaciÃ³n de operaciones.
- Esto hace que Spark sea mucho mÃ¡s rÃ¡pido que Hadoop MapReduce.

## ğŸ§© PySpark: DataFrames, schema inference y transformaciones
### Â¿CÃ³mo funciona DataFrame en PySpark?
Es una colecciÃ³n distribuida de datos organizados en columnas (como una tabla de SQL o un DataFrame de pandas) construido sobre RDDs, pero con optimizaciÃ³n automÃ¡tica gracias a Catalyst.
Spark puede inferir automÃ¡ticamente los tipos de datos (inferSchema=True) o puedes especificarlos manualmente con StructType.


| CaracterÃ­stica                | pandas                 | PySpark                            |
| ----------------------------- | ---------------------- | ---------------------------------- |
| Escalabilidad                 | Memoria local          | Procesamiento distribuido          |
| Velocidad en grandes datasets | Lenta                  | Muy rÃ¡pida (paralelismo + memoria) |
| Lazy evaluation               | âŒ No                   | âœ… SÃ­                               |
| Tipado                        | DinÃ¡mico               | Inferido o explÃ­cito               |
| Sintaxis                      | Muy expresiva/flexible | MÃ¡s declarativa y estricta         |
| Uso tÃ­pico                    | ExploraciÃ³n local      | Procesamiento a gran escala        |

### Â¿CuÃ¡ndo usar cada uno?
- Pandas: Ideal para datasets < 1 GB, exploraciÃ³n rÃ¡pida.
- PySpark: Fundamental para Big Data (decenas o cientos de GB), procesamiento distribuido, producciÃ³n.

---

# ğŸ§‘â€ğŸ’» Autor
Creado y documentado por MarÃ­a Casasola como parte de un proceso de aprendizaje profesional en ingenierÃ­a de datos.
Para dudas, puedes contactarme vÃ­a [LinkedIn](https://www.linkedin.com/in/mar%C3%ADa-casasola-calzadilla-303970184/).

## ğŸ¤ Contribuciones
Este repositorio estÃ¡ en constante mejora. Si tienes ideas, correcciones o quieres colaborar, Â¡haz un fork y crea un pull request!